
# ğŸ¥ Medical Telegram Data Warehouse

A scalable **data engineering & analytics pipeline** designed to extract, transform, enrich, and serve medical business data from Telegram channels using **asynchronous scraping**, **YOLOv8 computer vision**, a **dbt-powered PostgreSQL warehouse**, **FastAPI**, and **Dagster orchestration**.

---

## ğŸ“‚ Project Structure

```text
medical-telegram-warehouse/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚       â”œâ”€â”€ telegram_messages/   # JSON partitions (YYYY-MM-DD/channel.json)
â”‚       â””â”€â”€ images/              # Organized images (channel/msg_id.jpg)
â”œâ”€â”€ medical_warehouse/           # dbt project
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/             # Task 2: Data cleaning & casting
â”‚   â”‚   â””â”€â”€ marts/               # Task 2 & 3: Star schema & AI marts
â”‚   â”œâ”€â”€ tests/                   # dbt data quality tests
â”‚   â””â”€â”€ dbt_project.yml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scraper.py               # Task 1: Async Telegram scraper
â”‚   â”œâ”€â”€ to_postgres.py           # Task 2: Load JSON/CSV into PostgreSQL
â”‚   â””â”€â”€ yolo_detect.py           # Task 3: YOLOv8 object detection
â”œâ”€â”€ api/                         # Task 4: FastAPI application
â”‚   â”œâ”€â”€ main.py                  # API routes
â”‚   â”œâ”€â”€ database.py              # SQLAlchemy engine/session
â”‚   â””â”€â”€ schemas.py               # Pydantic models
â”œâ”€â”€ orchestration/               # Task 5: Dagster pipeline
â”‚   â””â”€â”€ pipeline.py              # Dagster jobs & schedules
â”œâ”€â”€ logs/                        # Scraper & pipeline logs
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md
```

---

## ğŸ›°ï¸ Task 1: Data Scraping & Collection (Extract)

A robust extraction layer for high-volume Telegram channels.

### 1ï¸âƒ£ Asynchronous MTProto Scraper

* Built with **Telethon** (MTProto protocol)
* Uses **asyncio** for concurrent message + media downloads
* Handles `FloodWaitError` gracefully to avoid bans

### 2ï¸âƒ£ Bronze Data Lake (Raw Layer)

* Raw data preserved to avoid early data loss
* **Hive-style partitioning**:

  ```
  data/raw/telegram_messages/YYYY-MM-DD/channel_name.json
  ```
* Images stored separately using message IDs for CV processing

---

## ğŸ—ï¸ Task 2: Data Modeling & Transformation (Transform)

Raw Telegram data is transformed into a clean **Star Schema** using **dbt + PostgreSQL**.

### 1ï¸âƒ£ Staging Layer (`stg_telegram_messages`)

* Casts ISO timestamps â†’ `TIMESTAMP`
* Normalizes engagement metrics with `COALESCE`
* Creates derived flags (e.g. `has_image`)

### 2ï¸âƒ£ Dimensional Modeling

* **fct_messages** â€“ engagement metrics & foreign keys
* **dim_channels** â€“ channel metadata & activity spans
* **dim_dates** â€“ date spine for time-series analytics

---

## ğŸ‘ï¸ Task 3: AI Enrichment (YOLOv8 Object Detection)

Images are enriched with **computer vision metadata**.

### 1ï¸âƒ£ Classification Logic

* **Promotional** â€“ person + product
* **Product Display** â€“ product only
* **Lifestyle** â€“ people without products

### 2ï¸âƒ£ Warehouse Integration

* YOLO detections stored in `detection.yolo_results`
* Joined via `fct_image_detections`
* Enables analysis of **human presence vs engagement**

---

## ğŸŒ Task 4: Analytical API (Serve)

A **FastAPI** service exposes warehouse insights via REST.

### 1ï¸âƒ£ Key Endpoints

* `GET /api/reports/top-products` â€“ most mentioned medical terms
* `GET /api/channels/{name}/activity` â€“ posting trends
* `GET /api/search/messages` â€“ keyword search
* `GET /api/reports/visual-content` â€“ AI image analytics

### 2ï¸âƒ£ Developer Experience

* **Pydantic** for strict validation
* **Swagger UI** available at `/docs`

---

## âš™ï¸ Task 5: Pipeline Orchestration (Automate)

The full workflow is automated using **Dagster**.

### 1ï¸âƒ£ Job Dependency Graph

```
Scrape Data
   â†“
YOLO Detection
   â†“
Load to PostgreSQL
   â†“
dbt Transformations
```

### 2ï¸âƒ£ Operations & Monitoring

* Daily scheduled runs
* Dagster UI for observability and debugging

---

## ğŸ’» How to Run

### 1ï¸âƒ£ Environment Setup

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

Create a `.env` file:

```env
API_ID=your_telegram_api_id
API_HASH=your_telegram_api_hash
DATABASE_URL=postgresql://user:password@localhost:5432/warehouse
```

---

### 2ï¸âƒ£ Run the Full Pipeline (Dagster)

```bash
dagster dev -f orchestration/pipeline.py
```

Access the Dagster UI:

```
http://localhost:3000
```

---

### 3ï¸âƒ£ Start the API

```bash
uvicorn api.main:app --reload
```

API Docs:

```
http://127.0.0.1:8000/docs
```

---

## âœ… Project Status

| Task               | Status      |
| ------------------ | ----------- |
| Task 1 â€“ Scraping  | âœ… Completed |
| Task 2 â€“ Warehouse | âœ… Completed |
| Task 3 â€“ AI (YOLO) | âœ… Completed |
| Task 4 â€“ FastAPI   | âœ… Completed |
| Task 5 â€“ Dagster   | âœ… Completed |

ğŸš€ **Full Production Pipeline: Active**

