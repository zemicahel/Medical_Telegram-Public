
---

# ğŸ¥ Medical Telegram Data Warehouse

A scalable **data engineering pipeline** designed to extract, transform, and analyze **medical business data from Telegram channels** using an asynchronous scraper, a **dbt-powered PostgreSQL warehouse**, and a modular analytics architecture.

---

## ğŸ“‚ Project Structure

```text
medical-telegram-warehouse/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚       â”œâ”€â”€ telegram_messages/  # JSON partitions (YYYY-MM-DD/channel.json)
â”‚       â””â”€â”€ images/             # Organized photos (channel/msg_id.jpg)
â”œâ”€â”€ medical_warehouse/          # dbt project directory
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/            # Task 2: Data cleaning & type casting
â”‚   â”‚   â””â”€â”€ marts/              # Task 2: Star Schema (Dimensions & Facts)
â”‚   â”œâ”€â”€ tests/                  # Custom data quality tests
â”‚   â””â”€â”€ dbt_project.yml         # dbt configuration
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scraper.py              # Task 1: Asynchronous Telegram scraper
â”‚   â””â”€â”€ to_postgres.py          # Task 2: JSON to PostgreSQL loader
â”œâ”€â”€ api/                        # Task 4: FastAPI Application (Roadmap)
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ logs/                       # Scraper activity & error logs
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md                   # Project documentation
```

---

## ğŸ›°ï¸ Task 1: Data Scraping & Collection (Extract)

The goal of this stage is to build a **robust extraction layer** capable of handling high-volume Telegram channels without triggering rate limits.

### 1ï¸âƒ£ Asynchronous MTProto Scraper

Using the **Telethon** library, the scraper interacts directly with Telegramâ€™s **MTProto protocol**, enabling efficient access to both messages and media.

* **Concurrency**
  Uses `asyncio` to download high-resolution images while parsing message metadata in parallel.

* **Resilience**
  Implements `FloodWaitError` handling to automatically pause and resume scraping when rate limits are encountered.

### 2ï¸âƒ£ Bronze Data Lake (Raw Layer)

All extracted data is stored in its **raw form** to avoid early data loss.

* **Hive-Style Partitioning**

  ```text
  data/raw/telegram_messages/YYYY-MM-DD/channel_name.json
  ```

  Enables partition pruning for downstream analytics and transformations.

* **Media Decoupling**
  Images are stored separately using message identifiers, keeping the warehouse lightweight while enabling downstream computer-vision analysis (Task 3).

---

## ğŸ—ï¸ Task 2: Data Modeling & Transformation (Transform)

Raw Telegram JSON is transformed into a **clean Star Schema** using **dbt** and **PostgreSQL**.

### 1ï¸âƒ£ Staging Layer (`stg_telegram_messages`)

Acts as the data cleaning factory:

* Casts ISO date strings to proper `TIMESTAMP`
* Normalizes engagement metrics using `COALESCE`
* Creates derived boolean fields (e.g. `has_image`)
* Ensures schema consistency before dimensional modeling

### 2ï¸âƒ£ Dimensional Modeling (Star Schema)

* **`fct_messages` (Fact Table)**
  Central table containing engagement metrics and foreign keys

* **`dim_channels` (Dimension)**
  Channel metadata including activity span and average engagement

* **`dim_dates` (Dimension)**
  Date spine enabling time-series analysis (weekday, weekend, quarter, etc.)

### 3ï¸âƒ£ Data Quality & Validation

A **test-driven transformation** approach using dbt tests:

* **Referential Integrity**
  Ensures every message maps to a valid channel

* **Custom Business Rules**
  Prevents data corruption using rules such as:

  * No future-dated messages
  * Valid timestamp ranges

---

## ğŸš€ Future Roadmap

### ğŸ‘ï¸ Task 3: Object Detection (YOLOv8)

Image analytics using **Ultralytics YOLOv8**:

* **Detection**
  Identify pills, bottles, and people in medical posts

* **Classification**

  * *Promotional*: Person + Product
  * *Product Display*: Product only

* **Goal**
  Measure how visual content type affects engagement

---

### ğŸŒ Task 4: Analytical API (FastAPI)

Expose the warehouse through a RESTful API.

Planned endpoints:

* `/api/reports/top-products`
* `/api/channels/{name}/activity`
* `/api/search`

Features:

* Pydantic-based request/response validation
* Read-optimized analytics queries

---

### âš™ï¸ Task 5: Pipeline Orchestration (Dagster)

Automate the end-to-end pipeline.

* Convert scripts into **Dagster ops**
* Schedule daily incremental runs
* Monitor pipeline health via Dagster UI

---

## ğŸ’» How to Run

### 1ï¸âƒ£ Environment Setup

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

---

### 2ï¸âƒ£ Data Extraction (Task 1)

Create a `.env` file containing your Telegram credentials:

```env
API_ID=your_api_id
API_HASH=your_api_hash
```

Run the scraper:

```bash
python src/scraper.py
```

---

### 3ï¸âƒ£ Load Data to PostgreSQL (Task 2)

```bash
python src/to_postgres.py
```

---

### 4ï¸âƒ£ dbt Transformations

```bash
cd medical_warehouse
dbt run
dbt test
```

View documentation:

```bash
dbt docs generate
dbt docs serve
```

---

## âœ… Project Status

